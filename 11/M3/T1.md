<h1 align="center">Tutorial: Multivariat Calculus og Gradienter</h1>

Efter denne tutorial vil du kunne:

*   Anvende **andenafledet-testen** til at klassificere stationære punkter (maksimum/minimum).
*   Bestemme **konkavitet** for en funktion.
*   Forstå funktioner af to variable samt finde deres **definitionsmængde** og **værdimængde**.
*   Beregne **partielle afledte** ved at holde en variabel konstant.
*   Opstille og beregne **gradientvektoren**.
*   Forstå gradientens geometriske betydning (retning for stejleste stigning).

## 1. Den Anden Afledede og Konkavitet

I forrige tutorial lærte vi at finde stationære punkter ved at sætte \(f'(x) = 0\). For at afgøre om et punkt er et maksimum eller minimum, brugte vi førsteafledet-testen (fortegnsskift). Vi kan også bruge den **anden afledede**, \(f''(x)\), som beskriver krumningen (konkaviteten) af grafen.

**Definitioner:**

*   **Konkav opad (Smilende):** Hvis \(f''(x) > 0\), krummer grafen opad (som en skål, der kan holde vand).
*   **Konkav nedad (Sur):** Hvis \(f''(x) < 0\), krummer grafen nedad (som en omvendt skål).
*   **Vendepunkt:** Et punkt hvor konkaviteten skifter (typisk hvor \(f''(x) = 0\)).

**Andenafledet-testen (The Second Derivative Test):**

Lad \(c\) være et stationært punkt (dvs. \(f'(c)=0\)).

*   Hvis \(f''(c) > 0\), har \(f\) et **lokalt minimum** i \(c\).
*   Hvis \(f''(c) < 0\), har \(f\) et **lokalt maksimum** i \(c\).
*   Hvis \(f''(c) = 0\), er testen inkonklusiv (brug da førsteafledet-testen).

**Eksempel:**

Find og klassificer stationære punkter for \(f(x) = x^3 - 3x\).

**Løsning:**

1.  Find \(f'(x)\) og sæt lig 0:
    $$f'(x) = 3x^2 - 3 = 0 \Rightarrow 3(x^2 - 1) = 0 \Rightarrow x = 1 \text{ eller } x = -1$$
2.  Find \(f''(x)\):
    $$f''(x) = 6x$$
3.  Test punkterne:
    *   Ved \(x = 1\): \(f''(1) = 6(1) = 6 > 0\) $\Rightarrow$ **Lokalt minimum**.
    *   Ved \(x = -1\): \(f''(-1) = 6(-1) = -6 < 0\) $\Rightarrow$ **Lokalt maksimum**.

## 2. Funktioner af Flere Variable

Indtil nu har vi arbejdet med funktioner af én variabel, \(f(x)\). I den virkelige verden afhænger resultater ofte af flere inputs (f.eks. afhænger huspriser af både størrelse og beliggenhed). Vi skriver en funktion af to variable som \(z = f(x, y)\).

**Definitionsmængde (Domain):**

Mængden af alle par \((x, y)\), som funktionen kan acceptere som input. For eksempel må tal under en kvadratrod ikke være negative.

**Eksempel på definitionsmængde:**

Bestem definitionsmængden for \(f(x, y) = \sqrt{16 - x^2 - y^2}\).

**Løsning:**

Udtrykket under kvadratroden skal være ikke-negativt:

$$16 - x^2 - y^2 \geq 0 \Rightarrow x^2 + y^2 \leq 16$$

Definitionsmængden er altså en cirkelskive med centrum i \((0,0)\) og radius 4.

## 3. Partielle Afledte

Når vi har en funktion af to variable, \(f(x, y)\), kan vi ikke bare "differentiere" den. Vi skal vide, hvilken retning vi ændrer os i. Vi bruger **partielle afledte** til at se ændringen i enten x-retningen eller y-retningen.

**Metode:**

*   For at finde \(\frac{\partial f}{\partial x}\) (den partielle afledede mht. \(x\)): Differentier mht. \(x\) og **behandl \(y\) som en konstant**.
*   For at finde \(\frac{\partial f}{\partial y}\) (den partielle afledede mht. \(y\)): Differentier mht. \(y\) og **behandl \(x\) som en konstant**.

**Eksempel:**

Find de partielle afledte af \(f(x, y) = 3x^2y + 5y^3 - 2x\).

**Løsning:**

1.  **Mht. \(x\) (\(\frac{\partial f}{\partial x}\)):**
    *   \(3x^2y\): \(y\) er konstant, så vi differentierer \(3x^2\) og ganger \(y\) på. Det giver \(6xy\).
    *   \(5y^3\): Dette led har ingen \(x\)'er, så det er en konstant. Den afledede af en konstant er 0.
    *   \(-2x\): Den afledede er \(-2\).
    
    Resultat: \(\frac{\partial f}{\partial x} = 6xy - 2\)

2.  **Mht. \(y\) (\(\frac{\partial f}{\partial y}\)):**
    *   \(3x^2y\): \(3x^2\) er konstant. Den afledede af \(y\) er 1. Det giver \(3x^2\).
    *   \(5y^3\): Den afledede er \(15y^2\).
    *   \(-2x\): Konstant mht. \(y\), så den afledede er 0.
    
    Resultat: \(\frac{\partial f}{\partial y} = 3x^2 + 15y^2\)

## 4. Gradienten

De partielle afledte fortæller os hældningen i x- og y-retningen. Hvis vi samler disse i en vektor, får vi **gradienten**, betegnet med \(\nabla f\) (nabla f).

**Definition:**

$$\nabla f(x, y) = \left\langle \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right\rangle = \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix}$$

**Vigtige egenskaber:**

1.  Gradienten peger i den retning, hvor funktionen **stiger hurtigst** (stejleste opstigning).
2.  Længden af gradientvektoren angiver, hvor stejl stigningen er.
3.  Gradienten står vinkelret på niveaukurver (contour lines).

**Eksempel:**

Find gradienten for \(f(x, y) = x^2 + 4y^2\) i punktet \((1, 1)\).

**Løsning:**

1.  Find de partielle afledte:
    \(\frac{\partial f}{\partial x} = 2x\)
    \(\frac{\partial f}{\partial y} = 8y\)

2.  Opstil gradienten generelt:
3.  
    $$\nabla f(x, y) = \begin{bmatrix} 2x \\ 8y \end{bmatrix}$$

4.  Indsæt punktet \((1, 1)\):
5.  
    $$\nabla f(1, 1) = \begin{bmatrix} 2(1) \\ 8(1) \end{bmatrix} = \begin{bmatrix} 2 \\ 8 \end{bmatrix}$$

Dette betyder, at i punktet (1,1) stiger funktionen hurtigst, hvis man bevæger sig i retningen af vektoren \(\begin{bmatrix} 2 \\ 8 \end{bmatrix}\).

## 5. Anvendelse: Gradient Descent

En af de vigtigste anvendelser af gradienten er inden for machine learning og optimering. Hvis vi vil finde minimum for en funktion (f.eks. minimere fejl i en model), og vi står på en "bjergside" (funktionen), gælder følgende:

*   \(\nabla f\) peger opad (stejleste stigning).
*   \(-\nabla f\) peger nedad (stejleste fald).

Algoritmen **Gradient Descent** fungerer ved iterativt at tage små skridt i retningen af \(-\nabla f\) for at finde bunden af dalen (minimum).

## Opsummering

*   **Andenafledet-testen:** \(f'' > 0\) betyder minimum (smilende), \(f'' < 0\) betyder maksimum (sur).
*   **Funktioner af to variable:** Tager \((x,y)\) som input. Definitionsmængden er de lovlige inputs.
*   **Partielle afledte:** Differentier mht. en variabel, mens de andre holdes konstante (opfattes som tal).
*   **Gradienten:** En vektor bestående af de partielle afledte \(\nabla f = \langle f_x, f_y \rangle\). Den peger mod stejleste stigning.

## Almindelige Faldgruber

*   **Konstanter ved partiel differentiation:** Når du differentierer mht. \(x\), skal du huske, at led som \(y^2\) eller \(5y\) forsvinder (bliver 0), fordi de er konstanter set fra \(x\)'s synspunkt. Led som \(x \cdot y\) bliver til \(1 \cdot y = y\).
*   **Andenafledet-testen:** Det er let at bytte rundt: Husk, at positiv \(f''\) (glad mund) betyder **minimum**, og negativ \(f''\) (sur mund) betyder **maksimum**.
*   **Gradienten er en vektor:** Resultatet af en gradient-beregning er ikke et tal, men en vektor (en liste af tal).
*   **Definitionsmængde:** Husk at udtryk under en kvadratrod skal være \(\geq 0\), og nævneren i en brøk må ikke være \(0\).